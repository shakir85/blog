[{"content":"This post is designed for those new to AWS, showing the steps to attach a new EBS volume and mount it in the operating system. The goal of mounting a data volume is to maintain a clear separation between your application data and the operating-system\u0026rsquo;s data.\nInitializing EBS volumes is important also when working with Auto Scaling Groups (ASG). Auto Scaling Groups use launch templates, which contain information such as the Amazon Machine Image (AMI), SSH keys, and more, to deploy new EC2 instances during scaling events. An AMI captures the state of the root volume and any additional attached EBS volumes, including their mount points. If you attach an EBS volume, mount it to a specific directory like /foo, and then create an AMI from that EC2 instance, the information about the attached EBS volume and its mount point will be preserved.\nRequrements AWS account (duh!) Adequate IAM permissions for EC2 and EBS (launch, attach/detach volumes). Basic familiarity with Linux storage mechanisms. Before we start, I\u0026rsquo;d like to mention that all the steps here can be automated using the AWS CLI/SDK, or your preferred Infrastructure as Code (IaC) tool. In this instance, we\u0026rsquo;ll walk through the console setup for clarity.\nDisclaimer This post will focus on provisioning a new volume, including the formatting process. Adding a new partition to a partially used volume is beyond the scope of this blog, and following the steps below will effectively wipe all the content from the specified volume.\nStep 1: Create an EBS volume Set your volume settings as you wish, just make sure it\u0026rsquo;s in the same AZ as the target EC2 instance.\nStep 2: Attache the EBS volume to your EC2 instance My running EC2 instance is called webserver and I will name the volume as /dev/sdb. This volume name should be reflected in the /dev directory once the volume is attached successfully.\nOnce attachment is successful, you should see the new volume in the main console\nStep 3: Configure the new volume When you add a new EBS volume to an EC2 instance in AWS, you generally need to follow similar steps as you would with on-premises servers. What we did above is just attaching a disk to a server, exactly like you would do on a physial server (or a volume to a VM in a type 1 hypervisor).\nSo we need to SSH into the server and do configure the volume.\nSSH into the EC2 Instance Make sure you have proper permissions to format, partition, and mount disks.\nIdentify the New Volume Run the lsblk (List Block Devices) command and list all disks on the system and try to identify the newly added volume\nThe lsblk command is handy for checking disk information, including where a disk is mounted. If you look at the output above, you\u0026rsquo;ll see that our recently added volume isn\u0026rsquo;t mounted yet. You might also wonder why it\u0026rsquo;s labeled as xvdb instead of the expected sdb. This change is due to a new naming system in the Linux kernel. Take a look at the highlighted box in the previous screenshot for more details\nInstall a partition table Once the disk is identified, we need to install a partition table.\nA partition table is a data structure on a storage device that defines the sections used for organizing data on the disk.\nRun the fdisk command and follow the instructions specfied in the code block below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ sudo fdisk /dev/sdb Welcome to fdisk (util-linux 2.37.4). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table. Created a new DOS disklabel with disk identifier 0xf8a58774. Command (m for help): g Created a new GPT disklabel (GUID: AC3E4A5C-F60F-5B42-85BB-B019A99EB6D0). Command (m for help): n Partition number (1-128, default 1): First sector (2048-209715166, default 2048): Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-209715166, default 209715166): Created a new partition 1 of type \u0026#39;Linux filesystem\u0026#39; and of size 100 GiB. Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. Enter g and hit enter, this will create a GPT partition table. Enter n and hit enter, this will create new partitio. Hit Enter, this will instruct fdisk to use all the free space in the disk to the new partition. Enter w and hit enter, this will write the new partition to the disk. Check device partitions The /dev directory is where device files are located, and devices like hard drives can have sequential names and additional entries representing partitions. For example, if you have a device named /dev/sdb, it might be the entire storage device. The partitions on that device are then represented by appending a number to the device name, such as /dev/sdb1, /dev/sdb2, and so on.\nSo now we have our disk device and it has one partition (from the previous step), we should see the following devices /dev/sdb and /dev/sdb1 in /dev\nCreate a file system Use the mkfs tool to create a file system on the new partition.\nThe -t option allows us to select the type of the file system; in this case, I will use ext4. The -L option can be any meaningful name for your volume. We will use that label in the LABEL option in the next step when we mount the file system, so be sure to use a helpful name.\nMount the new file system Create a new directory (or use an existing one), and mount the new file system to it\n1 $ sudo mount LABEL=ebs001 /ebs001 Check the mount point\n1 2 3 $ ls /ebs001 lost+found Let\u0026rsquo;s add a file to it for testing\nThe volume is now ready, allowing the EBS volume to easily be detached from its current EC2 instance and attached to another instance without any data loss. This volume is ideal for storing your application data, such as serving as a mount point for a Docker volume, which will keep the application data separated from the EC2 root volume at all times.\n","date":"2023-10-04T21:22:40-07:00","permalink":"https://demo.stack.jimmycai.com/p/mounting-aws-ebs-volumes/","title":"Mounting AWS EBS volumes"},{"content":"In Unix-like operating systems, an inode (short for \u0026ldquo;index node\u0026rdquo;) is a data structure that stores metadata about a file or directory. Each file or directory on a filesystem is associated with a unique inode which contains information such as:\nFile type (regular file, directory, symbolic link, device file, etc.)1. Permissions (read, write, execute) for the owner, group, and others. Ownership (user and group) of the file. File size in bytes. Timestamps indicate when the file was accessed and modified and when the inode itself was last modified2. The number of hard links to the file3. Disk block pointers point to the data blocks on the storage device. In this post, I\u0026rsquo;ll cover the basics of inodes – how to identify issues tied to them, and ways to troubleshoot. Keep in mind that inodes can become quite complex because they touch various parts of the filesystem.\nWhy the system needs something like inodes? When you create a file or directory, there is metadata associated with it, including: file name, size, type, permissions, owner, group, and more. The operating system needs something to manage the file\u0026rsquo;s metadata and the data-blocks location on the disk.\nTherefore, an inode is allocated to store the file (or directory) metadata, which allow the filesystem to properly manage file access and storage. This also make operations like finding files by name, checking permissions, and tracking file sizes easy for the OS.\nThis is in general. The specific implementation and terminology may vary slightly depending on the filesystem type and the OS version.\nInode capacity For a filesystem, there is a quota for its inodes. The number of inodes available on a filesystem is determined during the filesystem formatting and is usually fixed number. This means that if a filesystem runs out of available inodes, you may be unable to create new files or directories, even if there is a free disk space.\nIn inode, we trust! So, there is a capacity for inodes — great. But what happens if we run out of them?\nRunning out of inodes (inode exhaustion) can lead to weird system failures. You might suspect that the system has run out of available inodes when a program or process fails to create a file or directory due to insufficient storage, as the error message indicates.\nGenerally, it is unlikely to run out of inodes before running out of storage. But this is not impossible, especially when there are a lot of processes writing small files constantly for a long time.\nSo if you can\u0026rsquo;t create a file, and the error message states that you do not have enough space, even though you have disk space, then most likely you\u0026rsquo;ve run out of inodes.\nReasons for inodes exhaustion Inode exhaustion can be caused by (but not limited to) the following factors:\nSmall files and directories If you have a ton of very small files and directories, each will consume an inode, which will quickly deplete the available inode pool. Regardless of whether or not there is plenty of free space on the disk.\nLots of small writes Related to the previous point, frequent small writes, such as those caused by a lot of logging or temporary file creation, can contribute to inode exhaustion. These small writes create new inodes each time, and on the long run this can lead to a depletion of available inodes.\nTemporary files Applications that generate many temp files without cleanup can exhaust inodes. These temporary files accumulate and consume inodes if they are not frequently cleaned up.\nSoftware builds and compilation Software development processes involving frequent compilation and code builds can create many temp files and directories. For example, building NodeJS apps is known for creating a ton of small files and cache. Over time, these files can contribute to inode exhaustion.\nExcessive filesystem operations Certain applications or scripts that frequently create, modify, or delete files and directories can lead to inode exhaustion if not properly controlled.\nBundeld Servers Mail servers, content management servers (e.g., WordPress), and backup servers can also be reasons for running out of inodes. So, it\u0026rsquo;s a good practice to be aware of how these servers operate and how they interact with the file system.\nHow to fix inode exhaustion? First, ensure whether you’re running out of inodes or not: You might think that just restarting the server would fix the problem, but that\u0026rsquo;s not necesserly right. The issue is more about the filesystem itself and not the operating system processes.\nThe filesystem is persisted on disk, so a reboot does not modify the underlying filesystem properties. If the filesystem was already experiencing inode exhaustion before the reboot, the same issue will persist after the reboot.\nA reboot might be helpful only to clear out stale or stuck processes that keep trying to write tiny files. But the reboot itself does not directly fix the underlying filesystem issue.\nIdentify the issue Monitor filesystem usage and identify when inode exhaustion occurs. Review error messages, logs, and filesystem reports to determine the cause of the issue. If you encounter an error message indicating insufficient storage capacity, despite being certain that there is available space, it may suggest an inode issue. Cleanup and delete unnecessary files Identify and delete unnecessary files. Check users\u0026rsquo; files, and ensure temporary files are cleaned up. Check out the /tpm directory; applications usually use this location for scratch/cache data. Clear the cache of package managers by using their command line options or manually locate and remove their cache directory. Use tools like find to locate and delete no longer needed files. Clean up old log files, and ensure logrotate is enabled and properly configured. Storage optimization Consider compressing unused files. Remove redundant files. Avoid excessive nested directories. Consider using alternative storage like a NAS or other network filesystem. Prevent it from happening again Implement regular automated cleanup tasks. Reorganizing and tidying up the whole filesystem might be needed, this will need a storage expert. If you\u0026rsquo;re running into this problem often while compiling software in your CI/CD pipeline, you might want to think about using a Docker container to build the app and then stash away the build artifacts using Docker bind mounts or docker exporter. Then delete the build left overs. The first column of the left of the output of the ls -l command shows file types. The c, d are charcter and device files, l for symlinks, d for directoreis and - for files.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nUse stat command to view when a file created, accessed or modified as well as other file info.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nUnlike symlinks, which are references to file paths, hard links directly reference the underlying data blocks of a file on disk.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-08-16T23:17:40-07:00","permalink":"https://demo.stack.jimmycai.com/p/understanding-inodes/","title":"Understanding inodes"},{"content":"Using Amazon CloudFront to serve S3 data can be more cost-effective than serving data directly from S3 in some situations. In this post, I will discuss key points to consider when deciding to use CloudFront for serving S3 content at the edge.\nData transfer costs CloudFront can help reduce data transfer costs by caching and serving content from edge locations closer users. If your content is accessed frequently by users from different geographic locations, CloudFront can reduce the amount of data transferred over the internet compared to serving content directly from an S3 bucket.\nLatency and performance CloudFront caches content at edge locations, which can significantly reduce latency and improve the overall performance for users. Users receive content from the nearest edge location, reducing the round-trip time to the original S3 bucket.\nRequest patterns CloudFront can help reduce the number of requests made directly to your S3 bucket by handling a portion of the requests at the edge locations. This can be beneficial if you have a high volume of requests for the same content.\nEdge location data processing: Lambda@Edge, and CloudFront Functions CloudFront can also perform some data processing at the edge using Lambda@Edge, or CloudFront Functions. Lambda@Edge and CloudFront Functions allows us to add a computing element when serving content at edge. This includes customizing routing to S3 buckets, auth and auth, bot mitigation, serve specific content when your website is in maintenance mode, and overall improved user experience without modifying your website code.\nConsider these points too With all the mentioed features of CloudFront for serving S3 content at edge, it\u0026rsquo;s important to consider the following aspects as well:\nCloudFront cost While CloudFront can help reduce data transfer costs, it introduces its own costs based on the number of requests and data transferred from edge locations. If your data access patterns do not benefit from caching, i.e. requests always need to hit the backedn S3 bucket, then the return on investment (ROI) of using CloudFront for S3 content might not be as cost-effective as expected, and you might incur unnecessary additional costs.\nCache invalidation Content Delivery Networks (CDNs) are ideal for serving static content. They are not typically a good option for dynamic content mainly due to their caching and latency handling characteristics. However, this is changing rapidly. CDN providers nowadays offer features to address these limitations as the demand for globally accessed web applications is increasing. See this announcement from AWS in 2020.\nIf your data frequently changes and requires constant updates, you may need to have a good plan for managing cache invalidation and service costs.\nLow volume data transfer For very small data-transfers, or infrequent access patterns, the cost advantage of using CloudFront for serving data from S3 might be less significant compared to fetching the data from the bucket directly.\nTransfer to origin Data transfer from CloudFront to AWS services\u0026rsquo; origin is free, see the General section of CloudFront FAQ. However, it\u0026rsquo;s always a good idea to consult AWS documentation to understand any cost-related matters.\nConsider S3 Corss-Region Replication If your application does not benefit from caching but you still want to keep the S3 data close to your users (for READ only ops), then consider using S3 Corss-Region Replication instead of CloudFront.\n","date":"2023-08-01T11:03:48-07:00","permalink":"https://demo.stack.jimmycai.com/p/to-cloudfront-or-not-to-cloudfront/","title":"To CloudFront or not to CloudFront?"},{"content":"Whether it\u0026rsquo;s an on-prem firewall or in the cloud, it\u0026rsquo;s important to understand the fundamental distinctions in firewall types. In this post, I will summerize the differences between stateful and stateless filtering in simple and basic terms.\nStateful filtering Stateful filtering, is a firewall technique that tracks the state of network connections and makes decisions (such as allow/deny) based on the context of those connections. When a packet passes through a stateful firewall, the firewall keeps track of the state of the connection by creating an entry in a state table to monitor the connection\u0026rsquo;s status.\nThe state table contains information such as:\nSource\u0026rsquo;s IP address and port Destination\u0026rsquo;s IP address and port Connection status (e.g. established, new, related, or invalid). By maintaining this state information, the firewall can allow inbound traffic that is part of an outbound connection initiated from within the network. This \u0026ldquo;allow rule\u0026rdquo; does not have to be explicitly configured in the firewall to permit traffic from the destination back to the source. That\u0026rsquo;s whay they\u0026rsquo;re called \u0026ldquo;context-aware\u0026rdquo;, because this type is designed to understand the context of the conntection, where it\u0026rsquo;s coming from, where it\u0026rsquo;s going to, and who generated the connection.\nImagine your application sending a request, and when the destination sends the response back, the firewall just goes, \u0026lsquo;Oh, I\u0026rsquo;ve got this! It\u0026rsquo;s coming from a destination that a client from my network initiated the connection with. No need to have the destination IP in the allow-list; we\u0026rsquo;re all good to go!\u0026rsquo;.\nThe firewall will continue blocking traffic that does not correspond to an existing connection or violates the state table rules. Security Groups on major cloud providers typically operate in a stateful manner.\nAdvantages of Stateful Filtering Simplicity in implementation: No need to add an allow rule for the destination address if the connection originated from a client within the network. Improved security: Only allows traffic from established connections within our network. Stateless Filtering Stateless filtering, is a firewall technique that examines individual packets without considering the context or state of the connection to which they belong. Each packet is analyzed independently based on predefined filtering rules. If a client within the network sends a request to a remote destination, the destination address must be on the firewall\u0026rsquo;s allow list, otherwise the client won\u0026rsquo;t receive a response.\nThink of it like the customs kiosks at an airport in some country. They check arrivals and departures for travelers separately. The passengers\u0026rsquo; origin does not generally impact their work rules. All they know is whether a passenger is arriving or departing and whether they are permitted to enter or leave the country based on predefined rules.\nIn stateless filtering, the firewall evaluates each packet\u0026rsquo;s headers, such as source and destination IP, ports, and protocol type. If a packet matches one of the preconfigured rules, it is allowed or denied based on that rule\u0026rsquo;s criteria.\nStateless firewalls do not maintain a state table to track connection states, meaning they do not know established connections. As a result, they cannot dynamically allow response traffic for outgoing connections or handle traffic related to established connections.\nIn AWS, Network Access List (NACLs) operate in a stateless manner.\nAdvantages of Stateless Filtering Efficiency. Each packet is evaluated in isolation without the overhead of maintaining a state table. Useful for basic traffic filtering and access control based on packet headers. Scope of implementation The fundamental concepts of stateful and stateless firewalls are consistent across firewall appliance manufacturers as well as cloud providers. However, the implementation of stateless and stateful firewall functionality (i.e. how they do it) can vary.\nFor example, in AWS, stateful firewall functionality is implemented using Security Groups, which operate at the instance level. And stateless firewall functionality can be achieved through Network ACLs (Access Control Lists - NACLs), which operate at the subnet level.\nComparison Stateful filtering is more sophisticated and provides better security by considering the state of connections. It can dynamically allow responses to outgoing traffic and handle related traffic. Simplicity in usage: they\u0026rsquo;re good when simpler filtering rules based on static rules are sufficient. Finally In practice, firewalls often utilize both stateful and stateless filtering at various levels, such as subnet, or gateway, to provide complete security. Stateful filtering is commonly used for connection tracking, while stateless filtering allows for quick packet evaluation.\n","date":"2023-07-24T19:56:52-07:00","permalink":"https://demo.stack.jimmycai.com/p/firewalls-stateful-vs.-stateless/","title":"Firewalls: stateful vs. stateless"},{"content":"In this post, I will demonstrate how ENTRYPOINT and CMD work together, their differences, and how to redirect the runtime execution flow from ENTRYPOINT to the CMD where the main application\u0026rsquo;s command is executed.\nThe way ENTRYPOINT and CMD work together In most cases, CMD and ENTRYPOINT instructions can be used interchangeably. Also, you do not have to use both of them together in every Dockerfile you develop. However, each instruction offers additional features that can help you control how you want to run your application. Before moving forward, let\u0026rsquo;s quickly review what each instruction does:\nENTRYPOINT is like the \u0026ldquo;main command\u0026rdquo; or the starting point for your container. It\u0026rsquo;s the default action the container takes when you run it. You might use ENTRYPOINT to start a web server or run a specific application.\nCMD can be used to provide additional arguments or options to the command specified in ENTRYPOINT. It\u0026rsquo;s like saying, \u0026ldquo;When you start the container using the ENTRYPOINT command, here are some extra args to pass to the executing app\u0026rdquo;. It is often used to pass default arguments to ENTRYPOINT. Note that we said: \u0026ldquo;default arguments\u0026rdquo; which we\u0026rsquo;ll explain what does that mean in a bit.\nSo, ENTRYPOINT sets the main command of the container, and CMD provides default arguments to that command. Here is an example:\n1 2 3 FROM ubuntu ENTRYPOINT [\u0026#34;echo\u0026#34;] CMD [\u0026#34;Hello world\u0026#34;] Running a container from this Dockerfile is similar to executing echo \u0026quot;Hello world\u0026quot; in the command line. The echo is the main app and Hello world is the argument. Similarly, in the Dockerfile above, the content of the CMD instruction is passed to the ENTRYPOINT as the default argument. when we build and run the container without arguments it will print \u0026ldquo;Hello world\u0026rdquo;:\n1 2 docker build -t test . docker run test Output:\n1 Hello world Overriding CMD To override the CMD that is defined in the Dockerfile (default argument), you just pass the argument(s) after the image name:\n1 docker run test \u0026#39;another hello world\u0026#39; Output:\n1 another hello world This method also overrides the CMD whether it\u0026rsquo;s used in combination with ENTRYPOINT instruction or not.\nOverriding ENTRYPOINT Given this Dockerfile:\n1 2 FROM ubuntu ENTRYPOINT [\u0026#34;echo\u0026#34;, \u0026#34;Hello world\u0026#34;] When you have a Dockerfile with only an ENTRYPOINT (i.e. no CMD), you need to use the --entrypoint flag to override the entry-point command as the following:\n1 2 # docker run --entrypoint \u0026lt;command\u0026gt; \u0026lt;image\u0026gt; docker run --entrypoint \u0026#39;printenv\u0026#39; test Output:\n1 2 3 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=7ffd59696373 HOME=/root If you try to supply a command at runtime without specifying the --entrypoint flag, Docker will treat the that command as additional arguments to the command specified in the ENTRYPOINT:\n1 docker run test \u0026#39;printenv\u0026#39; Output:\n1 Hello world printenv This is similar to an entry-point in Dockerfile like this: ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;Hello world\u0026quot;, \u0026quot;printenv\u0026quot;]\nHanding over execution flow from ENTRYPOINT to CMD Consider the following Python Flask app:\n1 2 3 4 5 6 7 FROM python:latest # ... # RUN \u0026gt;\u0026gt;\u0026gt; install Python packages \u0026amp; configs ... # COPY \u0026gt;\u0026gt;\u0026gt; add files and executables # ... ENTRYPOINT [\u0026#34;uvicorn\u0026#34;] CMD [\u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;] The uvicorn command will be executed when the container runs, and the CMD instruction will provide the necessary arguments for the uvicorn server.\nUsually, we need a way to include runtime configurations that our Flask app expects to be available in the run environment prior to executing the main application in the entry-point (e.g. uvicorn). These configurations could be starting a service, exporting environment variables, running a database migration script, or simply editing certain configuration files.\nThis type of commands (runtime commands) cannot be included in RUN stages, and it is an anti-pattern and honestly quite ugly to cram a lot of shell commands into the ENTRYPOINT and/or CMD sections.\nEnter \u0026ldquo;docker-enterypoint.sh\u0026rdquo; When developing a Dockerfile, it is a common pattern to wrap various initialization commands within a shell script, conventionally named \u0026lsquo;docker-entrypoint.sh\u0026rsquo; or \u0026rsquo;entrypoint.sh\u0026rsquo; and execute it using an ENTRYPOINT instruction prior to running the main app. The purpose of this technique is to provide a flexible way of configuring the Docker container environment at run time.\nSince ENTRYPOINT instruction provides run-time execution, we need to find a way to return the execution flow back the Dockerfile\u0026rsquo;s CMD instruction to run the main application command.\nTo do so, simply add an exec \u0026quot;@$\u0026quot; statement at the very end of the shell script that is being executed by the ENTRYPOINT (i.e. \u0026lsquo;docker-entrypoint.sh\u0026rsquo;) file.\nAfter adding all configuration scripts to \u0026lsquo;docker-entrypoint.sh,\u0026rsquo; we will modify the Dockerfile as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 FROM python:latest # ... # RUN \u0026gt;\u0026gt;\u0026gt; install Python packages \u0026amp; configs ... # COPY \u0026gt;\u0026gt; add our app # .... # Copy the init script file to a directory in the PATH # You might need to `chmod +x` it too COPY docker-entrypoint.sh /usr/local/bin ENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;] CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;] To visualize the process:\nWhen we run the container, Docker will execute the ENTRYPOINT, which contains the \u0026ldquo;docker-entrypoint.sh\u0026rdquo; script. Then, the exec \u0026quot;$@\u0026quot; command in the \u0026ldquo;docker-entrypoint.sh\u0026rdquo; script will, in a sense, return control to the CMD. To clarify, the exec part won\u0026rsquo;t transfer execution flow; it just expands the arguments specified in the CMD instruction in a new process.\nLet\u0026rsquo;s break down what exec \u0026quot;@$\u0026quot; does:\nexec is a Linux command used to replace the current process with a new process. In this case, it ensures that \u0026quot;$@\u0026quot; becomes the main process running in the container. \u0026quot;$@\u0026quot; expands to all the command-line arguments passed to the container when it starts (e.g. expanding the content of the CMD instruction). It preserves the exact arguments that were passed during container runtime. Also, you still can override the CMD by specifying args on the docker run command. Finally, note that you cannot place any commands in the \u0026lsquo;docker-entrypoint.sh\u0026rsquo; file after the exec \u0026ldquo;$@\u0026rdquo; line. ","date":"2023-07-16T17:34:13-07:00","permalink":"https://demo.stack.jimmycai.com/p/docker-cmd-and-entrypoint-differences/","title":"Docker CMD and ENTRYPOINT differences"},{"content":"In this post, I will show you how to source (load) a Bash function from a local or remote source (e.g. a file in Github) into the current shell.\nIn shell scripting, using the source command (also known as the dot \u0026ldquo;.\u0026rdquo; command) allows to read and execute commands from a script file, and load its content into your current shell. This makes all variables, functions, and aliases defined in that script file become available in the current shell session.\nLoad from local file Similar to importing libraries in programming languages, you can organize your freqnetly used code in different files in your project directory and then load them as you need. See this example:\n1 2 3 4 5 6 7 8 9 10 # Project directory tree root ├── lib/ │ └── common.sh ├── var/ │ └── stuff.sh │ └── main_script.sh If you want to import a function from root/lib/common.sh to main_script.sh, you only need to source that file:\n1 2 3 4 5 6 #!/bin/bash # *** main_script.sh *** source lib/common.sh # . # code # . Load from remote file To load script content into your current shell without downloading the remote file, you can curl the content of the script and redirect it to a source command as the following (don\u0026rsquo;t forget the -s flag to silence curl\u0026rsquo;s download info):\n1 2 # You can use a dot `.` instead of `source` as well $ source \u0026lt;(curl -s https://raw.githubusercontent.com/shakir85/utils/main/print_hello) Remote file content:\n1 2 3 4 #!/bin/bash print_hello() { echo \u0026#34;This is the boring hello world message\u0026#34; } After that, you can invoke the print_hello function from your current shell:\n1 2 $ print_hello This is the boring hello world message This technique allows us to set environment variables, import functions, or modify the current shell\u0026rsquo;s behavior using the contents of a remote script. This is a cool trick when you do not want to persist the data of the remote script on the host or want to load functions or variables to the current working shell on the fly. The downside, though, is that if the remote content vanishes, your script could bust!\n","date":"2023-06-22T08:34:22-07:00","permalink":"https://demo.stack.jimmycai.com/p/importing-functions-in-shell-scripting/","title":"Importing functions in shell scripting"},{"content":"This post will cover the definition of Helm\u0026rsquo;s building blocks and basic commands.\nWhy Helm? When we intend to deploy an application to a K8s cluster, it\u0026rsquo;s important to know that Kubernetes doesn\u0026rsquo;t inherently understand our application\u0026rsquo;s requirements (and it shouldn\u0026rsquo;t).\nWhat I mean by that is Kubernetes doesn\u0026rsquo;t recognize that PVC \u0026ldquo;x\u0026rdquo; is associated with Deployment \u0026ldquo;y,\u0026rdquo; which, in turn, relies on Service \u0026ldquo;z\u0026rdquo;, and without these components, the application will not run as intended. All what Kubernetes knows is to manage its resources and thrive on keeping cluster\u0026rsquo;s objects alive.\nSo we needed a way to \u0026ldquo;bundle\u0026rdquo; our Kubernetes application for better release management. We wanted to put the pieces together. Out API app needs a Deployment that requires a Service and we need a storage for that. So we all these parts should deployed, removed, updated, versioned, and shared together, becasue these parts represent a specific application.\nHelm is designed to know about our application. That\u0026rsquo;s why it\u0026rsquo;s called a package manager because it looks at our application\u0026rsquo;s K8s manifests as a group (package). It allows us to look at our Kubernetes application as \u0026ldquo;an application\u0026rdquo; rather than a collection of Kubernetes objects.\nHelm Chart A Helm Chart is a packaged, deployable unit that comprises all configurations needed for deploying an application to a Kubernetes cluster.\nLet\u0026rsquo;s break this statement down:\nPackaged: A Helm chart is a package for Kubernetes applications, similar to how a Debian apt package bundles software for easy installation on Linux.\nDeployable unit: A Helm chart can be deployed to a K8s cluster, similar to the other K8s objects. It has all\nIncludes all configuration files: Within a chart, you\u0026rsquo;ll add all the required K8s manifests (such as service, deployment, replicaset, etc.), that your application needs. Making it easy to deploy and manage our application within a Kubernetes cluster.\nBy combining these three characteristics, you can deliver your application to any Kubernetes cluster of your choice easily. Not only that, but you can also share your chart (i.e. your applications) so others can deploy it to their cluster too.\nHelm Release A Helm \u0026ldquo;release\u0026rdquo; is a running instance of a Helm chart. When you install a Helm chart, it generates a release with a unique name and version. You can install a chart multiple times, each will have its unique name, and optinally, you can use different variables and parameters.\nFor example, when you install an Nginx chart, the installed instance of the chart is called a \u0026ldquo;release\u0026rdquo;. You can have one or more Nginx releases deployed from the same chart; each installation has its unique name.\nHelm Repository A \u0026ldquo;repository\u0026rdquo; in Helm refers to a storage location where Helm charts are stored. Like any artifact repository, it can be accessed and shared. It can be a remote or local.You can interact with Helm repositories using the helm repo \u0026lt;command\u0026gt; to add, update, or manage repositories.\nHelm Hub A \u0026ldquo;hub\u0026rdquo; refers to a centralized platform that hosts a collection of repositores such as the default one Artifact Hub (artifacthub.io). These hubs provide a user-friendly interface for discovering and exploring Helm repositories.\nHelm Hub vs. Repo Think of Helm repositories like storage places where Helm charts are stored and Helm hub is like a platform that offers a search service to find Helm charts from different repositories. So basically, Helm Hub makes it easy to find and interact with Helm charts, while Helm repositories store the actual chart files.\nCommands cheat sheet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 # Helm works similarly to Linux package managers: # 1. You add, update, or delete a repository, # 2. Install, and delete Helm charts. # # By default, Helm uses https://artifacthub.io for charts. # # Helm deploys K8s resources, this means that Helm is aware of K8s namespaces. # Helm will always use the default namespace of the currently activated context # in your K8s cluster. # Similar to kubectl, use the \u0026#39;-n\u0026#39; flag to interact with K8s namespaces # while working with Helm commands. # # Search the default Helm hub helm search hub \u0026lt;chartName\u0026gt; # Add a repo to your local helm setup helm repo add bitnami https://charts.bitnami.com/bitnami # Refresh helm packages list helm repo update # Show added repos helm repo list # Search for a chart in the previously added repos. # The `CHART VERSION` shows the chart\u0026#39;s version, and # the `APP VERSION` column shows the application\u0026#39;s version (e.g. Nginx\u0026#39;s version). # This command will list the latest version available in the repository. helm search repo \u0026lt;chartName\u0026gt; # List all `APP VERSION` for a specific package helm search repo --versions \u0026lt;chartName\u0026gt; # Install a chart: # You can install the same chart multiple times with different names. # Chart names must be unique. A running instance of a chart is called `release`. # Sticking to a naming scheme for your releases is always a good idea. helm install [specify-release-name] [chart-name] helm install my-release-1 bitnami/nginx helm install my-release-2 bitnami/nginx -n dev-namespace # List installed packages helm list helm ls helm ls -a # list `all` including failing deployments helm ls --all-namespaces # Uninstall a chart helm uninstall my-release-1 # Download (+ unpack) the chart but don\u0026#39;t install it helm pull --untar bitnami/wordpress # Once downloaded, you can install the chart helm install [release-4] ./\u0026lt;chart\u0026gt; helm install my-release ./wordpress # Install a specific version of a chart (this is `CHART VERSION`) helm install [release] [repo/chart] --version [version] helm install my-release bitnami/nginx --version 9.9.4 # Upgrade a release to the latest version available in the repo. # use `--version` followed by chart version number to upgrade to # a certain version. helm upgrade [release] [chart] helm upgrade my-release bitnami/nginx # You can \u0026#39;downgrade\u0026#39; using the same command but with an older `--version` number helm upgrade my-release bitnami/nginx --version 9.9.0 # The upgrade command also takes variables updating flags. # Here, we are changing the default service type of Nginx # from LoadBalancer to a NodePort and the default value of # the port from 80 to 8181 helm upgrade my-release --set service.type=NodePort --set service.ports.http=8181 bitnami/nginx # To retain the previously modified values when upgrading a release helm upgrade my-release --version 9.9.5 --reuse-values bitnami/nginx # Show details of a chart without installing it helm show chart bitnami/nginx # Helm `get` to pull extended info about a release # you can get release\u0026#39;s -\u0026gt; \u0026#39;manifest\u0026#39;, \u0026#39;values\u0026#39;, \u0026#39;notes\u0026#39;, \u0026#39;hooks\u0026#39;, \u0026#39;all\u0026#39; helm get manifest [releaseName] helm get values [releaseName] helm get all [releaseName] # Get status of a release (deplyed, failed ...etc) helm status [release] # List all available values in a chart without downloading it. # if you\u0026#39;re looking for specific flag, pipe the output to grep i.e. \u0026#39; | grep \u0026#39; helm show values [repo/chart] helm show values bitnami/apache | grep replicaCount # Then to update a default variable, use the \u0026#39;--set\u0026#39; flag helm install apache-release bitnami/apache --set replicaCount=2 # Control the output using -o [options] helm search hub nginx-ingress -o yaml ","date":"2023-05-30T12:54:12-07:00","permalink":"https://demo.stack.jimmycai.com/p/helm-basics/","title":"Helm basics"},{"content":"Having multiple network interfaces on one machine can be pretty handy. It gives you network backup and helps you bounce back if the network gets a bit erratic. So, in this post, I\u0026rsquo;ll walk you through setting up a \u0026lsquo;default gateway\u0026rsquo; on one interface to handle outbound traffic (internet), while keeping the second one reserved just for LAN networking. Also, I\u0026rsquo;ll share some quirky bits I had to figure out along the way\nSetting the stage I have a mini PC (ThinkCentre M710q) running Debian 11. This device is equipped with two network interfaces: one is an Ethernet port, and the other is a Wi-Fi device.\nI also have access to two totally different networks. So, this arrangement allows the host to connect to two different, publicly routable IP addresses via two different gateways. My plan is to connect use ethernet port for local networking, and use the wifi interface for internet traffic. To make this happen, we just need to make sure the host always selects the wifi-interface\u0026rsquo;s gateway as the default gateway.\nGateway and interfaces configuration First, make sure that each interface is connected to its corresponding network and has been assigned a DHCP IP from its respective gateway.\n1. Identify the interfaces Open a terminal window and run ip address show or ip a for short:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 user@host:~$ ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp0s31f6: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 6q:4b:40:29:2x:e1 brd ff:ff:ff:ff:ff:ff inet 10.10.50.36/24 brd 10.10.50.255 scope global dynamic noprefixroute enp0s31f6 3: wlp2s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 1a:3b:70:31:51:63 brd ff:ff:ff:ff:ff:ff inet 172.20.13.25/16 brd 172.20.255.255 scope global dynamic noprefixroute wlp2s0 valid_lft 70380sec preferred_lft 70380sec In the output above I have two active interfaces; both are up, and each has been assigned a DHCP IP:\nEthernet interface: enp0s31f6 Wifi interface: wlp2s0 2. Identify the default gateway for each interface We need to know each gateway\u0026rsquo;s IP address to set up the default gateway on the host. We can find that using the ip route command\n1 2 3 user@host:~$ ip route default via 10.10.50.10 dev enp0s31f6 proto dhcp metric 100 default via 172.20.1.1 dev wlp2s0 proto dhcp metric 600 From the output above, each interface has the following information\nInterface Gateway Assiged DHCP IP enp0s31f6 10.10.50.10 10.10.50.36 wlp2s0 172.20.1.1 172.20.13.25 3. Reset the default gateway The operating system currently uses 10.10.50.10 as the default gateway. To switch the default gateway to 172.20.1.1, we need to delete the default gateway and then set the second one as the default gateway.\nSince each gateway represents a publicly routable IP address, let\u0026rsquo;s take note of the current public IP address on the host before updating the gateways:\n1 2 user@host:~$ curl ifconfig.me \u0026lt;Output=Network-1-Public-IPv4\u0026gt; Now let\u0026rsquo;s delete the current default gateway 10.10.50.10:\n1 user@host:~$ ip route del default Set the default gateway to the interface we want (wifi interface):\n1 user@host:~$ ip route add default via 172.20.1.1 dev wlp2s0 Check the public IP address again (it should return the second network\u0026rsquo;s public IP address):\n1 2 user@host:~$ curl ifconfig.me \u0026lt;Output=Network-2-Public-IPv4\u0026gt; Verify gateway change using tcpdump You can use tcpdump to verify that the public traffic has been re-routed to the second gateway (wifi NIC).\ntcpdump is a command line tool used to capture network traffic in real-time. It is a widely-used tool for troubleshooting networks and analyzing network activity.\nIn our case, tcpdump can capture packets that are sent and received through the wifi NIC on the secondary gateway. This allows us to gather detailed information about each packet, including the source and destination addresses. Consequently, we can determine whether the host utilizes the wifi-NIC\u0026rsquo;s gateway as the default gateway.\nRun tcpdump -D as a root user to list active interfaces:\n1 2 3 root@host:~# tcpdump -D 1.enp0s31f6 [Up, Running, Connected] 2.wlp2s0 [Up, Running, Wireless, Associated] Run tcpdump -i wlp2s0 -n -nn and inspect the output, note that we are using the -i flag to target the wifi interface:\n1 2 3 4 5 6 7 8 9 root@host# tcpdump -i wlp2s0 -n -nn [1] 11:54:37.300783 ARP, Reply 172.20.1.1 is-at 00:50:e8:04:5f:73, length 46 [2] 11:54:37.300785 ARP, Reply 172.20.1.1 is-at 00:50:e8:04:5f:73, length 46 [3] 11:54:37.300786 ARP, Reply 172.20.1.1 is-at 00:50:e8:04:5f:73, length 46 [4] 11:54:37.300786 ARP, Reply 172.20.1.1 is-at 00:50:e8:04:5f:73, length 46 [5] 11:54:37.529574 IP \u0026lt;Network-IP\u0026gt;.43681 \u0026gt; 146.70.172.2.18748: UDP, length 176 [6] 11:54:37.881405 IP 146.70.172.2.18748 \u0026gt; \u0026lt;Network-IP\u0026gt;.43681: UDP, length 384 [7] 11:54:42.530855 IP \u0026lt;Network-IP\u0026gt;.43681 \u0026gt; 146.70.172.2.18748: UDP, length 176 [8] 11:54:42.737942 IP 146.70.172.2.18748 \u0026gt; \u0026lt;Network-IP\u0026gt;.43681: UDP, length 384 Note: the \u0026lt;Network-IP\u0026gt; is the redacted wifi-NIC\u0026rsquo;s public IP address.\nFrom the output, we can identify the following:\nLines 1 to 4 show that the interface communicates with the default gateway we configured previously. Lines 5 to 8 show that the interface\u0026rsquo;s local IP and a remote destination are talking to each other. Now let\u0026rsquo;s do the same thing with the ethernet interface:\n1 2 3 4 5 6 7 8 9 10 root@host# tcpdump -i wlp2s0 -n -nn 12:07:30.306031 IP 10.10.50.36.22 \u0026gt; 10.10.50.209.40122: Flags [P.], seq 1088064413:1088064609, ack 4151802530, win 501, options [nop,nop,TS val 703782894 ecr 4187823825], length 196 12:07:30.306385 IP 10.10.50.209.40122 \u0026gt; 10.10.50.36.22: Flags [.], ack 196, win 18695, options [nop,nop,TS val 4187823851 ecr 703782894], length 0 12:07:30.395515 IP 10.10.50.36.22 \u0026gt; 10.10.50.209.40122: Flags [P.], seq 196:568, ack 1, win 501, options [nop,nop,TS val 703782984 ecr 4187823851], length 372 12:07:30.395740 IP 10.10.50.209.40122 \u0026gt; 10.10.50.36.22: Flags [.], ack 568, win 18695, options [nop,nop,TS val 4187823940 ecr 703782984], length 0 12:07:30.499285 IP 10.10.50.36.22 \u0026gt; 10.10.50.209.40122: Flags [P.], seq 568:916, ack 1, win 501, options [nop,nop,TS val 703783088 ecr 4187823940], length 348 12:07:30.499721 IP 10.10.50.209.40122 \u0026gt; 10.10.50.36.22: Flags [.], ack 916, win 18695, options [nop,nop,TS val 4187824044 ecr 703783088], length 0 12:07:30.603219 IP 10.10.50.36.22 \u0026gt; 10.10.50.209.40122: Flags [P.], seq 916:1264, ack 1, win 501, options [nop,nop,TS val 703783192 ecr 4187824044], length 348 12:07:30.603720 IP 10.10.50.209.40122 \u0026gt; 10.10.50.36.22: Flags [.], ack 1264, win 18695, options [nop,nop,TS val 4187824148 ecr 703783192], length 0 12:07:30.707004 IP 10.10.50.36.22 \u0026gt; 10.10.50.209.40122: Flags [P.], seq 1264:1612, ack 1, win 501, options [nop,nop,TS val 703783295 ecr 4187824148], length 348 The output above may seem verbose due to an ongoing TCP communication, with tcpdump displaying all the steps involved in the TCP connection process.\nThe interface\u0026rsquo;s IP: 10.10.50.36 is engaged in communication with a local destination on the same subnet: 10.10.50.209, and vice versa, over port 22. (This is an active SSH tunnel from my laptop to the device.) The tcpdump output is not showing any active WAN communication on this interface.\nMission accomplished! Now all WAN traffic is routed through gateway 172.20.1.1, and the ethernet interface that\u0026rsquo;s connected to gateway 10.10.50.10 is only available for LAN connections.\nSome gotchas During boot-time network configuration, a race condition is likely to occur regarding which interface the operating system will utilize to set the default gateway.\nBoot time network configuration is all about getting a host\u0026rsquo;s network interfaces ready to communicate when the OS is firing up. Basically, it\u0026rsquo;s setting up things like IP address, the network essentials, including the gateway address, so your device can link up with the network and chat with other devices. The operating system figures out the default gateway based on the first NIC that obtains an IP address and gateway info from its own gateway.\nIn my experience, the OS always seems to prefer the Ethernet interface when it\u0026rsquo;s setting up the default gateway. I think this is due to the fact that Ethernet uses dedicated physical cables for communication, while the wifi interface relies on wireless signals, which can be affected by interference and signal strength. This can make wifi take a couple of seconds to catch up.\nRegardless, there are two solutions to remedy such a situation:\n1. Use /etc/rc.local or systemd/rc-local.service Basically, add the \u0026ldquo;delete-gateway\u0026rdquo; -\u0026gt; \u0026ldquo;set-gateway\u0026rdquo; commands explained above to the /etc/rc.local. The /etc/rc.local is a script file that is executed by the Linux init system during the boot process. The commands or scripts in the file are executed with root privileges, so it is important to use caution when modifying the file.\nNote that the /etc/rc.local file is deprecated in some Linux distributions, such as Ubuntu and Debian, in favor of systemd. Systemd uses its own mechanism for executing scripts and services at boot time, and the equivalent of the /etc/rc.local file in systemd is the /etc/systemd/system/rc-local.service file.\n2. Use the good ol\u0026rsquo; cron Add a CRON schedule to run the \u0026ldquo;delete-gateway\u0026rdquo; -\u0026gt; \u0026ldquo;set-gateway\u0026rdquo; commands. This approach might introduce some network interruption when CRON is triggered. So wrapping these commands in a shell script with some if-else logic would be a good idea to check if the gateway has changed during boot time before updating it.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash SECOND_NETWORK_GATEWAY=\u0026#34;172.20.1.1\u0026#34; CURRENT_PUBLIC_IP=\u0026#34;$(curl ifconfig.me)\u0026#34; SECOND_NETWORK_PUBLIC_IP=\u0026#34;99.99.99.99\u0026#34; INTERFACE_ID=\u0026#34;wlp2s0\u0026#34; if [ \u0026#34;$CURRENT_PUBLIC_IP\u0026#34; != \u0026#34;$SECOND_NETWORK_PUBLIC_IP\u0026#34; ] ; then ip route del default ip route add default via \u0026#34;$SECOND_NETWORK_GATEWAY\u0026#34; dev $INTERFACE_ID fi Signing off for now And that\u0026rsquo;s a wrap for now! Until the next post, keep on exploring, learning, and enjoying Linux networking. Catch you on the flip side! 🚀👋\n","date":"2023-05-13T21:34:49-07:00","permalink":"https://demo.stack.jimmycai.com/p/control-the-default-gateway-in-a-dual-nic-host/","title":"Control the default gateway In a dual NIC host"},{"content":"The scratch base is a Docker\u0026rsquo;s reserved blank image, or an empty filesystem, that acts like an empty layer to create parent images. It is like an empty canvas. It\u0026rsquo;s where you start building containers from scratch (no pun intended!), adding only what your application needs, making it super minimal. This gives us complete control over what can be shipped inside the container.\nIn this post, I will show you two different ways to utilize the \u0026lsquo;scratch\u0026rsquo; base. The first part will explore how to create minimal Docker images primarily for sharing files with other images and use container hubs, like ECR and Docker Hub, as file storage. In the second part, I will discuss the advantages of using the scratch base layer for deploying single-binary applications.\nSharing files between images When building images, Docker gives us the ability to pull files from other images (remote or local) using the --from= option with the COPY instruction in Dockerfile as follows:\n1 2 3 4 5 FROM ubuntu:latest COPY --from=foo:1.2 /content /content # Other build commands ... What\u0026rsquo;s neat about this is that it enables us to cherry-pick specific files from another image and toss them into our new image while it\u0026rsquo;s building. And the cherry on top? You can even pick files from a specific image by specifying in its tag. So if you have two tags for the image foo: foo:latest and foo:1.2, you can pull files from the version 1.2 on the fly.\nTreat your container hub as a remote storage Since we can copy files from remote images into a Dockerfile to include them in new image builds, we can actually store project files in the container registry as container images. You might wonder, why would you do that? Why not just use object storage like AWS S3 or even a Git repo to store and fetch files dynamically?\nWell, it\u0026rsquo;s just an additional option that comes with its own set of benefits:\nYou don\u0026rsquo;t need to fuss with remote storage authentication especially if you\u0026rsquo;re already logged in to your container registry. You\u0026rsquo;re already authenticated, which is super handy in CI/CD pipelines.\nIt brings reproducibility to the table. Every image in your pipeline can fetch files from a single source (image) that the pipeline is already has access to. This consistency makes it easy to replicate builds.\nBut, be aware that poorly planning how you use this approach can turn it into a dependency hill, and you might end up shooting yourself in the foot. So, use it wisely and be sure to document your approach.\nSo, if your intention is to use Docker images solely for storing files, then here\u0026rsquo;s the approach you should take:\nDockerfile content:\n1 2 3 4 5 6 # Use scratch image, you don\u0026#39;t need a distro FROM scratch # Copy all files you want to share from other images COPY somescript.sh /content COPY somearchive.tar.gz /content Build the image:\n1 docker build -t foo:1.2 . Push to remote (skip if you want the image to be local)\n1 docker push shakir85/foo:1.2 Then, copy the files from the remote container registry into your Docekrfile:\n1 2 3 4 5 6 7 8 # This is your application image FROM ubuntu:latest # Get files from remote image COPY --from=shakir85/project_files:latest /content /content # Build your image # ... Although you can achieve the same result with minimal images like Alpine or Busybox, using a distro-based image solely for file storage and sharing in Docker is not as efficient as using scratch base image.\nUse sctach base for single binary containers The scratch base layer can be an excellent choice for creating single-binary containers when your application and its dependencies are entirely self-contained within a single executable file.\nWhen you use FROM scratch, you start with an empty filesystem, and you can add only what is absolutely necessary for your application to run. This approach can help produce minimal container with a very small footprint because it contains only your application binary and nothing else.\nThe catch is that, since the scratch layer is essentially an empty filesystem, your application must be statically compiled. Also, keep in mind that because your application is going to be statically compiled, a small-sized container is not guaranteed. The container\u0026rsquo;s size really depends on the type and requirements of the application and the number of libraries or dependencies that need to be included (compiled) along with the application.\nThat being said, let\u0026rsquo;s take a look at this simple hello-world C code:\n1 2 3 4 5 6 #include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;hello world\u0026#34;); return 0; } Compile it using --static flag to include the required libraries in the final executable:\n1 gcc -o hello --static hello.c Create the Dockerfile:\n1 2 3 FROM scratch COPY hello / CMD [ \u0026#34;/hello\u0026#34; ] Build the image and run the container:\n1 2 docker build --no-cache -t my-scratch:latest . docker run --rm my-scratch If we try to send an echo command to the container, it will fail because there is no such a binary or application in the scratch container\n1 2 3 4 5 docker run --rm my-scratch echo hi docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \u0026#34;echo\u0026#34;: executable file not found in $PATH: unknown. Bonus Say, for example, we want to add the echo command to the scratch container. Since echo is a compiled binary, we may think we can copy it from another parent image into the scratch image using COPY --from=ubuntu:latest /usr/bin/echo / in the Dockerfile.\nHowever, since echo is a dynamically linked binary, the echo binary will need some dependencies in order to run. We can use the ldd command1 to view what libraries echo depends on. Let\u0026rsquo;s jump into an Ubuntu container and examine that:\n1 2 3 4 5 6 7 8 docker run -it --rm ubuntu:latest bash root@cd3dd0afeb53:/# which echo /usr/bin/echo root@cd3dd0afeb53:/# ldd /usr/bin/echo linux-vdso.so.1 (0x00007ffe99d81000) libc.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fecf34c3000) /lib64/ld-linux-x86-64.so.2 (0x00007fecf36f9000) The output shows the echo command\u0026rsquo;s dependencies that must be in the container, which without them, the echo command will not work.\nThis Reddit post shows some interesting facts about ldd.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-04-28T15:23:17-07:00","permalink":"https://demo.stack.jimmycai.com/p/a-practical-approach-for-using-docker-scratch-base-layer/","title":"A practical approach for using Docker scratch base layer"},{"content":"A Docker container is a process, isolated from the host and other containers, running on the system using various Linux kernel features, such as namespaces and cgroups. And this is the main differentiator between VMs and containers.\nWhen you start a Docker container, Docker creates a new process in the host system\u0026rsquo;s process tree. Then it will apply the container\u0026rsquo;s configuration such as its file system, network settings and so on to this process.\nThis makes the host OS consider a Docker container as just another process running on the system. Since the container is running as a process, we can actually use standard process monitoring tools, such as ps and top, to view and manage Docker.\nIn this blog post, we will examine how to find and access a container\u0026rsquo;s process ID (PID) and root filesystem directly from the host machine.\nGetting Started\u0026hellip; Let\u0026rsquo;s spin up a container and tinker with it\n1 docker run -d --name nginx nginx:latest Access container PID Docker stores detailed information about the container, including its image, configuration, volume, process ID, and network, in a low-level JSON object. You can use the docker inspect command, pipe the output to jq to parse the JSON object as you wish. Alternatively, you can query a scalar element by its name using Go language template syntax, as follows:\n1 2 # SYNTAX: docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; \u0026lt;CONTAINER_ID|CONTAINER_NAME\u0026gt; docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; nginx Check the /proc directory In Linux, the /proc directory is a virtual file system that provides a view of the system\u0026rsquo;s running processes. It contains files and directories that are dynamically generated by the kernel to provide information about the processes, hardware, and other system information.\nEach process running on the system has its own subdirectory under /proc, identified by its process ID (PID). For example, if you have a process id = 12345, you\u0026rsquo;d find its subdirectory in this path: /proc/12345. Inside the PID subdirectory (e.g. /proc/12345 in our case), you can find various files that provide information about the process, such as its memory usage, file descriptors, and more.\nSo, since the Nginx container that we spun up previously is just a process, we should see a directory named after its PID in /proc.\nLet\u0026rsquo;s re-run the above command and assign the output to a variable, amd ls its proc directory:\n1 2 PID=$(docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; nginx) ls /proc/$PID The output contains everything related to the container process. Explore the cgroup or environ files. Feel free to inspect the other files as well.\nNow let\u0026rsquo;s inspect the container\u0026rsquo;s \u0026ldquo;root\u0026rdquo; filesystem /proc/$PID/root\n1 2 3 ls /proc/$PID/root bin dev docker-entrypoint.sh home lib64 mnt proc run srv tmp var boot docker-entrypoint.d etc lib media opt root sbin sys usr If we exec into the container, we can see the same content from inside the container\n1 2 3 4 docker exec -it nginx sh root@ed08325bda2d:/# ls bin dev\tdocker-entrypoint.sh home lib64 mnt proc run srv tmp var boot docker-entrypoint.d etc\tlib media opt root sbin sys usr Manipulate the container process Like any process on the host, you can control it, but with some limitations. You can see below how the container was terminated using the kill command without interacting with the Docker daemon.\n1 2 3 4 5 6 7 8 9 10 11 12 ps aux | grep $PID root 8929 0.0 0.0 8936 5872 ? Ss 11:30 0:00 nginx: master process nginx -g daemon off; root 9053 0.0 0.0 17864 2408 pts/4 S+ 11:31 0:00 grep --color=auto 8929 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 522e39dfc08e nginx \u0026#34;/docker-entrypoint.…\u0026#34; 10 minutes ago Up 7 minutes 80/tcp nginx kill -9 $PID docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Conclusion We explored how to find a container\u0026rsquo;s process ID and how to access its root filesystem from the host. Unlike virtual machines, containers are isolated processes running in the host. This approach allows Docker to provide lightweight, efficient containerization that can be easily managed and monitored using standard Linux tools. It also allows Docker to run on a wide variety of Linux systems, without requiring any special kernel modifications or configurations.\n","date":"2023-04-22T11:17:28-07:00","permalink":"https://demo.stack.jimmycai.com/p/docker-containers-process/","title":"Docker containers process"}]